{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"PyTorch tutorial, by Jason Liu jasonl@wustl.edu for CSE527A\"\"\"\n",
    "# reference: https://pytorch.org/tutorials/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import libraries\"\"\"\n",
    "import torch # pytorch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataset\"\"\"\n",
    "from torch.utils.data import dataset, dataloader\n",
    "class CustomDataset(dataset.Dataset):\n",
    "    \"\"\"Contains data to train the model. \n",
    "    An instance of this object is created separately for training, validation, and testing\"\"\"\n",
    "    def __init__(self, data=[[1, 2, 3], [4, 5, 6]]): # They can be more and other arguments\n",
    "        super().__init__() # this needs to be here, because we're using inheritance\n",
    "        \"\"\"This object inherits from the Dataset base class,\n",
    "        To use its functionarlity, we need to overwrite its member functions as needed! (below)\n",
    "        We can also add additional methods\"\"\"\n",
    "        # This is a good time to make your data part of this class\n",
    "        self.data = data\n",
    "    def __getitem__(self, index): # The name matters, e.g. getitem() doesn't work!\n",
    "        \"\"\"How this dataset extracts items by index\n",
    "        'Index' is often one number, but doens't have to be!\"\"\"\n",
    "        # It's a good idea to turn your data into Tensors, so they can work with the model below\n",
    "        return 0, 0 # The return value can be a bunch of things (a tuple)\n",
    "        # For example, in supervised learning, this would be (data, label)  \n",
    "    def __len__(self):\n",
    "        \"\"\"Other objects that work with the dataset might need this function\n",
    "        Usually, this is the number of elements in the dataset\"\"\"\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Dataloader\"\"\"\n",
    "\"\"\"Feeds data into the model. \n",
    "The data comes from your dataset class, but they are is stacked up in batches\n",
    "Similar to Dataset, we need one of these for training, validation, and testing!\n",
    "Usually, we don't need to make a superclass of Dataloader\"\"\"\n",
    "train_loader = dataloader.DataLoader(dataset=CustomDataset(), batch_size=10)\n",
    "# By default, when we specify a batch_size, the dataloader gives a stack of 'batch_size' number of data in\n",
    "#   each step of being enumerated\n",
    "validation_loader = dataloader.DataLoader(dataset=CustomDataset())\n",
    "test_loader = dataloader.DataLoader(dataset=CustomDataset())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Model\"\"\"\n",
    "class CustomModel(nn.Module):\n",
    "    \"\"\"Contains neural network model\"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"The definition of your model: its layers and settings\"\"\"\n",
    "        super().__init__()\n",
    "        # For example, the nn.Sequential() container can pack up a bunch of layers\n",
    "        #   When called, data is passed through these layers sequentially\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(in_features=1, out_features=1),\n",
    "            nn.Linear(in_features=1, out_features=1) \n",
    "        )\n",
    "        # Of course, it's not necessary to use sequential layers\n",
    "        # Note that we can have a CustomModel2(nn.Module(2)) and use it here,\n",
    "        #   so we get a model-in-model situation.\n",
    "        #   This is helpful for styling or code readability. \n",
    "        #   Importantly, that CustomModel2 need to have a forward() method defined!\n",
    "    def helper(self, something): # Optional, name doesn't matter\n",
    "        return something\n",
    "    def forward(self, input): # Name matters! The model needs this to work\n",
    "        \"\"\"How the model calculates predictions\n",
    "        You do NOT call this method, because it's called by the nn.Module's __call__ function\"\"\"\n",
    "        # It's NOT ok for other methods to have this name\n",
    "        # If your code gets messy, some calculation can be off-loaded to other methods\n",
    "        return self.network(self.helper(input))\n",
    "m = CustomModel() # Create an instance of your model to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loss function\"\"\"\n",
    "loss_function = nn.CrossEntropyLoss() # This one of many loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Optimizer\"\"\"\n",
    "learning_rate = 1e-3 # A hyperparameter you tune\n",
    "# Optimizer's job is to optimize the model's parameters\n",
    "# So we need to put the model's parameters into the optimizer\n",
    "optimizer = torch.optim.Adam(params=m.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Training model\"\"\"\n",
    "def train(dataloader, model, loss_function, optimizer):\n",
    "    # It's not necessary for the train() method to have the above interface\n",
    "    for index, data in enumerate(dataloader):\n",
    "        # Enumerate() goes through what's in the dataloader one by one\n",
    "        # 'Data' is what we're getting each time. It's one of many items, \n",
    "        #   with the shape (batch size, shape of data straight out of your dataset's __getitem__)\n",
    "        # 'Index' is the index of that data\n",
    "        #   It's not necessary to use 'index,' but it might be useful if you want to print the progress of your training\n",
    "        #   For example:\n",
    "        \"\"\"1. Get prediction from model\"\"\"\n",
    "        # You might do something to the data so it fits in the model (e.g. change its shape)\n",
    "        prediction = model(data) # we're using the forward() function here\n",
    "        \"\"\"Also: It's a good idea to be specific (fully-spelled-out words) in naming your varaibles\"\"\"\n",
    "        #   1 people who read your code, e.g. TAs helping you to debug, or your future self trying to recycle code, understand it faster\n",
    "        #   2 in VScode, you rarely need to type more than 10 characters to auto-fill your desired varaible name with the Tab button\n",
    "        #   Fast way to get help: (we can quickly see what's what and start trouble-shooting!)\n",
    "        #       stuff_i_get_from_model = m(data) <- wordy, but helpful (you're good!)\n",
    "        #       predictions = m(data) <- concise and helpful (perfect!)\n",
    "        #       very_long_name_separated_by_underscode = m(data) <- it's a good idea to leave the details as a comment\n",
    "        #   Slow way to get help: (because we're spending time to understand what is what)\n",
    "        #       pred = m(data) <- it's getting confusing, I need to take a closer look\n",
    "        #       p = m(data) <- I need time to figure this out, so\n",
    "        #       randomletters = m(data) <- I need a lot of time to guess-work or ask you for clarification\n",
    "        #       someRandomNameSeparatedByCapitalLetters = m(data) <- I'm running into a parsing error\n",
    "        \"\"\"2. Calculate the loss\"\"\"\n",
    "        loss = loss_function(prediction) # the loss of the model's prediction\n",
    "        # In the supervised learning setting, we often see something like loss_function(prediction, label)\n",
    "        #   where label is from 'data' from dataloader\n",
    "        \"\"\"3. Optimization\"\"\"\n",
    "        # It's unlikely that you need any code sandwitched between these 3 following lines\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        # Sometimes, we evaluate the model too, see test() below for how it's done\n",
    "        # Validation is usually done along with training\n",
    "        #   so it makes sense for that process to stay in this function\n",
    "        #   i.e. we usually don't see a validation() function\n",
    "        # It's a good idea to record the loss in some way, so we can plot it for evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Testing the model\"\"\"\n",
    "def test(dataloader, model, loss_function):\n",
    "    \"\"\"The following code can be used for validation too,\n",
    "    beacuse when we validate, we also don't want to optimize the model and don't need the gradients\"\"\"\n",
    "    # The training step EXCEPT here's no optimization\n",
    "    for index, data in enumerate(dataloader):\n",
    "        with torch.no_grad(): # MUST to be here\n",
    "            \"\"\".no_grad() is a context manager\n",
    "            Its job is to tell the model to NOT have gradients in the calculation within its context\n",
    "            This matters because when we're testing, we don't need the gradients\"\"\"\n",
    "            prediction = model(dataloader)\n",
    "            loss = loss_function(prediction)\n",
    "            # When testing, we certainly don't want to optimize the model, \n",
    "            #   because that changes the parameters' values\n",
    "    # It's a good idea to record the loss in some way, so we can plot it for evaluation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Actually training the model\"\"\"\n",
    "number_of_epochs = 10 # a hyperparameter to tune\n",
    "for epoch in range(number_of_epochs):\n",
    "    train(train_loader, m, loss_function, optimizer)\n",
    "    test(validation_loader, m, loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0], dtype=torch.int32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"Debugging tensor-related errors\"\"\"\n",
    "# 1 Shape error\n",
    "# Make sure to read the documentation of objects you use to understand what kind of shape they want\n",
    "# For example, if some input goes in 'first_layer' then 'second_layer', \n",
    "#   the out_features of the 'first_layer' need to equal the in_features of 'second_layer'\n",
    "first_layer = nn.Linear(in_features=10, out_features=10)\n",
    "second_layer = nn.Linear(in_features=20, out_features=12)\n",
    "# 2 Gradient error\n",
    "# Parameters of PyTorch models are tensors, and they require gradients to get optimized\n",
    "# This can be specified when greating tensors\n",
    "tensor_with_gradient = torch.tensor([0.1], requires_grad=True) # ok to be the parameters of a model\n",
    "# On the other hand, data you feed into the model do not get optimized,\n",
    "#   so you don't need to worry about giving them gradients!\n",
    "tensor_just_data = torch.tensor([0.1])\n",
    "# 3 Device error\n",
    "# Two tensors need to be on the same device to interact (e.g. be in the same calculation)\n",
    "# You can change the device of a tensor using .device()\n",
    "device_you_want_tensor_to_be_on = 'mps' # on Colab, this should be 'cuda'\n",
    "tensor_just_data.to(device_you_want_tensor_to_be_on)\n",
    "# Being on the right device is also important for model calculation speed\n",
    "# If the device is CPU, we're missing out on the speed-up from using cuda CPU!\n",
    "m.to(device_you_want_tensor_to_be_on)\n",
    "# 4 Data type error\n",
    "# Some layers have specific requirements about the data type of the tensor\n",
    "# We can check the datatype of our tensors by .dtype()\n",
    "tensor_just_data.dtype\n",
    "# We can change the datatype using .to(PyTorch datatype)\n",
    "tensor_just_data.to(torch.int32) # note that we enter torch.data_type, not just data_type"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
